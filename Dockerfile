# ğŸš€ æœ€æ–°é…ç½®ï¼šCUDA 12.8 + é ç·¨è­¯ Flash Attention
FROM nvidia/cuda:12.8.1-cudnn-runtime-ubuntu22.04

WORKDIR /app

# 1ï¸âƒ£ ç³»çµ±ä¾è³´
RUN apt-get update && apt-get install -y --no-install-recommends \
    python3.10 python3-pip wget curl ca-certificates \
    && ln -sf /usr/bin/python3.10 /usr/local/bin/python \
    && ln -sf /usr/bin/pip3 /usr/local/bin/pip \
    && apt-get clean && rm -rf /var/lib/apt/lists/*

# 2ï¸âƒ£ å‡ç´š pip
RUN pip install --no-cache-dir --upgrade pip setuptools wheel

# 3ï¸âƒ£ å®‰è£ PyTorchï¼ˆä½¿ç”¨ cu124ï¼Œèˆ‡ Flash Attention åŒ¹é…ï¼‰
RUN pip install --no-cache-dir \
    torch==2.5.1 \
    torchvision==0.20.1 \
    torchaudio==2.5.1 \
    --index-url https://download.pytorch.org/whl/cu124

# 4ï¸âƒ£ ã€é—œéµã€‘ä¸‹è¼‰ä¸¦å®‰è£é ç·¨è­¯çš„ Flash Attention
RUN wget -q https://github.com/Dao-AILab/flash-attention/releases/download/v2.8.3/flash_attn-2.8.3+cu124torch2.5cxx11abiFALSE-cp310-cp310-linux_x86_64.whl \
    && pip install --no-cache-dir flash_attn-2.8.3+cu124torch2.5cxx11abiFALSE-cp310-cp310-linux_x86_64.whl \
    && rm flash_attn-2.8.3+cu124torch2.5cxx11abiFALSE-cp310-cp310-linux_x86_64.whl

# 5ï¸âƒ£ é©—è­‰æ‰€æœ‰çµ„ä»¶
RUN python -c "\
import torch; \
import torchvision; \
import flash_attn; \
print('='*70); \
print('ğŸ“¦ çµ„ä»¶ç‰ˆæœ¬'); \
print('='*70); \
print(f'âœ… PyTorch: {torch.__version__}'); \
print(f'âœ… TorchVision: {torchvision.__version__}'); \
print(f'âœ… Flash Attention: {flash_attn.__version__}'); \
print(f'âœ… CUDA å®¹å™¨ç‰ˆæœ¬: 12.8.1'); \
print(f'âœ… PyTorch CUDA ç‰ˆæœ¬: {torch.version.cuda}'); \
print('\\n' + '='*70); \
print('ğŸ”§ CUDA ç‹€æ…‹'); \
print('='*70); \
print(f'âœ… CUDA å¯ç”¨: {torch.cuda.is_available()}'); \
if torch.cuda.is_available(): \
    print(f'âœ… æ”¯æ´çš„æ¶æ§‹: {torch.cuda.get_arch_list()}'); \
    print(f'âœ… BFloat16 æ”¯æ´: {torch.cuda.is_bf16_supported()}'); \
else: \
    print('âš ï¸  CUDA ä¸å¯ç”¨ï¼ˆæ§‹å»ºæ™‚æ­£å¸¸ï¼Œé‹è¡Œæ™‚éœ€è¦ --gpus allï¼‰'); \
print('='*70); \
"

# 6ï¸âƒ£ æ‡‰ç”¨ä¾è³´
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# 7ï¸âƒ£ æ‡‰ç”¨ä»£ç¢¼
COPY . .

# 8ï¸âƒ£ ç’°å¢ƒè®Šé‡ï¼ˆæ”¯æ´ Blackwell åŠä»¥ä¸‹æ‰€æœ‰æ¶æ§‹ï¼‰
ENV PYTHONUNBUFFERED=1 \
    PYTHONDONTWRITEBYTECODE=1 \
    CUDA_VISIBLE_DEVICES=0 \
    TORCH_CUDA_ARCH_LIST="8.0;8.6;8.9;9.0;10.0" \
    TRANSFORMERS_CACHE=/app/.cache/huggingface

# 9ï¸âƒ£ å‰µå»ºç·©å­˜ç›®éŒ„
RUN mkdir -p /app/.cache/huggingface

# ğŸ”Ÿ é root ç”¨æˆ¶
RUN useradd -m -u 1000 appuser && \
    chown -R appuser:appuser /app
USER appuser

# 1ï¸âƒ£1ï¸âƒ£ å¥åº·æª¢æŸ¥
HEALTHCHECK --interval=30s --timeout=10s --start-period=60s --retries=3 \
    CMD curl -f http://localhost:8003/health || exit 1

EXPOSE 8003

# 1ï¸âƒ£2ï¸âƒ£ å•Ÿå‹•æœå‹™
CMD ["python", "-m", "uvicorn", "app.main:app", \
     "--host", "0.0.0.0", "--port", "8003", \
     "--workers", "1", \
     "--timeout-keep-alive", "300"]